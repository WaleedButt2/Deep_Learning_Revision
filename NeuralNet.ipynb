{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error,accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we do it for football.csv file\n",
    "df = pd.read_csv('./titatnic.csv')\n",
    "df=df.drop('Passengerid',axis=1)\n",
    "df=df.dropna(axis=0)\n",
    "df.corr()\n",
    "y = df.pop('2urvived').to_numpy()\n",
    "df=df.drop(['zero','zero.1','zero.2','zero.3','zero.4','zero.5','zero.6','zero.7','zero.8','zero.9','zero.10','zero.11','zero.12','zero.13','zero.14','zero.15','zero.16','zero.17','zero.18'],axis=1)\n",
    "X = df.to_numpy() \n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "        return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_sizes=[7,5,3]\n",
    "weights=[]\n",
    "biases=[]\n",
    "for i in range(1, len(layer_sizes)):\n",
    "    weights.append(np.random.rand(layer_sizes[i], layer_sizes[i-1]))  # Weights for each layer\n",
    "    biases.append(np.random.rand(layer_sizes[i])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1045, 7) (7,) ()\n",
      "(1045, 7) (7,) ()\n",
      "(1045, 7) (7,) ()\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(biases)):\n",
    "    print(X_train.shape,weights[i].shape,biases[i].shape)\n",
    "    z[i] = np.dot(X, weights[i]) + biases[i]# find z for the ith layer\n",
    "    a[i] = sigmoid(z[i]) # find a of the ith layer\n",
    "    x = a[i] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet:\n",
    "    def __init__(self,layer_sizes, learning_rate):\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        for i in range(1, len(layer_sizes)):\n",
    "            self.weights.append(np.random.rand(layer_sizes[i], layer_sizes[i-1]))  # Weights for each layer\n",
    "            self.biases.append(np.random.rand(layer_sizes[i]))  # Bias for each layer\n",
    "        self.learning_rate = learning_rate\n",
    "        self.z = [None] * len(self.biases)\n",
    "        self.a = [None] * len(self.biases)\n",
    "    def forward(self, x):\n",
    "        for i in range(len(self.biases)):\n",
    "            print(x.shape,self.weights[i].shape,self.biases[i].shape)\n",
    "            self.z[i] = np.dot(x, self.weights[i]) + self.biases[i]# find z for the ith layer\n",
    "            self.a[i] = self.sigmoid(self.z[i]) # find a of the ith layer\n",
    "            x = self.a[i]  #set input as a of this layer so the next layer uses previous layers activations as input\n",
    "        return self.a[-1]\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.forward(x)\n",
    "\n",
    "    def optimize(self, x, y):\n",
    "        n=len(y)\n",
    "        for i in reversed(range(len(self.weights))):\n",
    "            if i == len(self.weights) - 1:\n",
    "                # For the output layer, use the direct error\n",
    "                error = self.a[i] - y\n",
    "            else:\n",
    "                # For hidden layers, propagate the error from the next layer\n",
    "                error = np.dot(self.weights[i + 1], error)\n",
    "\n",
    "            if i != 0:\n",
    "                dw = np.dot(self.a[i-1].T, error) / n\n",
    "                db = np.sum(error, axis=0) / n\n",
    "            else:\n",
    "                # For the first layer (i = 0), use the input features 'x' instead of previous activations\n",
    "                dw = np.dot(x.T, error) / n\n",
    "                db = np.sum(error, axis=0) / n\n",
    "            self.weights[i] -= self.learning_rate * dw\n",
    "            self.biases[i] -= self.learning_rate * db\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def relu(z):\n",
    "        return np.maximum(0,z)\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid(z):\n",
    "        return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1045, 7)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1045, 7) (5, 7) (5,)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (1045,7) and (5,7) not aligned: 7 (dim 1) != 5 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[100], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(steps):\n\u001b[0;32m----> 4\u001b[0m     \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     nn\u001b[38;5;241m.\u001b[39moptimize(X_train,y_train)\n\u001b[1;32m      6\u001b[0m     pred\u001b[38;5;241m=\u001b[39mnn\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "Cell \u001b[0;32mIn[97], line 14\u001b[0m, in \u001b[0;36mNeuralNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbiases)):\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28mprint\u001b[39m(x\u001b[38;5;241m.\u001b[39mshape,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights[i]\u001b[38;5;241m.\u001b[39mshape,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbiases[i]\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 14\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mz[i] \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbiases[i]\u001b[38;5;66;03m# find z for the ith layer\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ma[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msigmoid(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mz[i]) \u001b[38;5;66;03m# find a of the ith layer\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ma[i]  \u001b[38;5;66;03m#set input as a of this layer so the next layer uses previous layers activations as input\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (1045,7) and (5,7) not aligned: 7 (dim 1) != 5 (dim 0)"
     ]
    }
   ],
   "source": [
    "nn=NeuralNet(layer_sizes=[X_train.shape[1],5,3],learning_rate=0.1)\n",
    "steps=10\n",
    "for i in range(steps):\n",
    "    nn.forward(X_train)\n",
    "    nn.optimize(X_train,y_train)\n",
    "    pred=nn.predict(X_test)\n",
    "    y_pred = np.where(pred >= 0.5, 1, 0)\n",
    "    print(accuracy_score(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
